
<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <meta name="keywords" content="eccv, workshop, computer vision, natural language processing, computer graphics, visual learning, machine learning">

  <link rel="shortcut icon" href="static/img/site/favicon.png">

  <title>GenAIMAGIC@CVPR25</title>
  <meta name="description" content="2nd GenAI Media Generation Challenge, CVPR 2025 Workshop">

  <!--Open Graph Related Stuff-->
  <meta property="og:title" content="2nd GenAI Media Generation Challenge Workshop"/>
  <meta property="og:url" content="https://gamgc.github.io"/>
  <meta property="og:description" content="2nd GenAI Media Generation Challenge, CVPR 2025 Workshop"/>
  <meta property="og:site_name" content="2nd GenAI Media Generation Challenge Workshop"/>
  <meta property="og:video" content="https://gamgc.github.io/static/img/site/teaser_video.mp4"/>

  <!--Twitter Card Stuff-->
  <meta name="twitter:card" content="summary_large_image"/>
  <meta name="twitter:title" content="2nd GenAI Media Generation Challenge Workshop"/>
  <meta name="twitter:video" content="https://gamgc.github.io/static/img/site/teaser_video.mp4">
  <meta name="twitter:url" content="https://gamgc.github.io"/>
  <meta name="twitter:description" content="2nd GenAI Media Generation Challenge, CVPR 2025 Workshop"/>

  <!-- CSS  -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
  <link rel="stylesheet" href="static/css/main.css" media="screen,projection">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>


  <style>
    .highlight-link {
        background-color: yellow;
        text-decoration: underline;
        padding: 2px 4px;
        border-radius: 3px;
    }
    .people-pic {
      max-width: 139px;
      max-height: 139px;
      /*width:300px;*/
      /*height:300px;*/
      object-fit: cover;
      border-radius: 50%;
  }

    .speaker-pic {
      max-width: 150px;
      max-height: 150px;
      /*width:300px;*/
      /*height:300px;*/
      object-fit: cover;
      border-radius: 50%;
  }
  </style>
</head>

  <body>

    <!-- <div class="top-strip"></div> -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="container">

    <div class="navbar-header">
      <a class="navbar-brand" href="/"></a>
      <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>

    <div class="navbar-collapse collapse" id="navbar-main">
      <ul class="nav navbar-nav">
        <li class="dropdown">
          <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Past Workshops <span class="caret"></span></a>
          <ul class="dropdown-menu">
            <li><a href="cvpr2024/index.html" target="__blank">CVPR 2024</a></li>
          </ul>
        </li>
        <li><a href="#intro">Challenge Overview</a></li>
        <li><a href="#submission">Submission</a></li>
        <li><a href="#dates">Dates</a></li>
        <!-- <li><a href="#winners">Winners</a></li> -->
        <li><a href="#dates">Schedule</a></li>
        <li><a href="#speakers">Invited Speakers</a></li>
        <li><a href="#organizers">Organizers</a></li>
        <li><a href="#contact">Contact</a></li>
      </ul>
    </div>

  </div>
</div>


    <div class="container">
      <div class="page-content">
          <p><br /></p>
<div class="row">
  <div class="col-xs-12">
    <center><h1>2nd GenAI Media Generation Challenge Workshop @ CVPR2025</h1></center>
    <center>Jun 11th (9:00 - 12:00)</center>
    <center>Workshop Meeting Location: 110A</center>
  </div>
</div>

<hr />


<div class="alert alert-info" role="alert">
  <center><b>Join Zoom Meeting  <a href="">TBA</a>.</b></center>
</div>


<div class="row" id="teaser">
    <div>
    <center>
      <video with="720" height="520" autoplay muted controls loop>
    <source src="static/img/site/teaser_video.mp4", type="video/mp4">
  </video>
  </center>
  </div>
</div>

<p><br /></p>
<div class="row" id="intro">
  <div class="col-xs-12">
    <h2>Workshop Overview</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">


    <p>
    This year, we are excited to host the 2nd GenAI Media Generation Challenge Workshop at CVPR 2025. Building on the success of last year's event, which focused on text-to-image and image editing tasks, we are expanding the challenge to include video generation.
    </p>
    <p>
    We are proud to announce the launch of the 2nd GenAI Media Generation Challenge (MAGIC), featuring a media generation track and auto-evaluation track:
      </p>
      <ul>
        <li>
          <b>Media Generation Festival:</b> For the first time, we are organizing a media generation festival with no restrictions on prompts. We would define a few different topics for which submitted media would compete in, and participants can submit their best generated videos or images for those specific topics. For each topic, we run a crowd sourced voting mechanism to determine the winners for each topic .
        </li>
        <li>
          <b>Auto Evaluation Challenge:</b> We are introducing an auto evaluation challenge for both text-to-image and text-to-video tasks. Participants can develop and submit their auto evaluation score for a preselect set of images and videos that we will provide and enter into the media generation festival track. Auto evaluation submissions would be to predict the outcomes from the crowd sourced voting mechanism in the media generation festival  The auto evaluation method that achieves the best correlation with the final results would be the winners for this challenge..
        </li>
      </ul>
    </p>

    <p><br /></p>


    <style>
      .submit-button {
        background-color: #3e8e41;
        color: white;
        padding: 10px 20px;
        text-decoration: none;
        border-radius: 5px;
        transition: background-color 0.3s ease-in-out;
      }
      .submit-button:hover {
        background-color: #2f6f31;
      }
    </style>
    <div style="background-color: #4CAF50; padding: 10px; text-align: center; border-radius: 5px;">
      <h2 style="color: white;">Crowedsourcing voting platform" is Open!</h2>
      <p style="color: white;">
        Click the link below to vote for your favorite AI-generated media.
      </p>
      <a href="https://sekunde.pythonanywhere.com/" class="submit-button" target="_blank">Vote Now</a>
    </div>
    <p><br /></p>
    <div style="background-color: #4CAF50; padding: 10px; text-align: center; border-radius: 5px;">
      <h2 style="color: white;">Submission for "Challenge A - Media Generation Festival" is Open!</h2>
      <p style="color: white;">
        Click the link below to share your generated images or videos with us. See <a href="#challenge_a_instruction"><b>Submission Instruction</b></a>.
      </p>
      <a href="https://forms.gle/ufkUb381GPW1X41j7" class="submit-button" target="_blank">Submit Now</a>
    </div>
    <p><br /></p>
    <div style="background-color: #4CAF50; padding: 10px; text-align: center; border-radius: 5px; margin-top: 10px;">
      <h2 style="color: white;">Submission for "Challenge B - Auto Evaluation" is Open!</h2>
      <p style="color: white;">
        Click the link below to share your auto eval results with us. See <a href="#challenge_b_instruction"><b>Submission Instruction</b></a>.
      </p>
      <a href="https://forms.gle/R7BHXDyJy6JHkzzQ9" class="submit-button" target="_blank">Submit Now</a>
    </div>
    <p><br /></p>



    <h2> Challenges Overview </h2>

    <H4>Challenge A -  Media Generation Festival</H4>
      <H5>Track A.1 - Video (Short)</H5>
      <p>
        For this task, there are no restrictions on prompts or models but limited to based on topics. Participants are free to use any models, including third-party video generation tools, and are encouraged to design their own creative prompts to produce engaging videos. Submissions can be single topic submissions or multiple topic submissions. However, the video length is limited to a maximum of 10 seconds.
      </p>
      <p>
        For this track, we will have the following set of of topics:
      </p>
      <ol>
        <li>
          People
        </li>
        <li>
          Animals
        </li>
        <li>
          Landscape
        </li>
      </ol>

      <H5>Track A.2 - Video (Long)</H5>
      <p>
        Similar to track A.1, there are no restrictions on prompts or models but limited to based on topics. For this track though, the video length is limited to a maximum of 5 minutes.
      </p>
      <p>
        For this track, we will have the following set of of topics:
      </p>
      <ol>
        <li>
          Action
        </li>
        <li>
          History
        </li>
        <li>
          Sci-Fi
        </li>
        <li>
          Fantasy
        </li>
        <li>
          Comedy
        </li>
      </ol>

      <H5>Track A.3 - Images</H5>
      <p>
        For this task, there are no restrictions on prompts or models but limited to based on topics. Participants are free to use any models, including third-party video generation tools, and are encouraged to design their own creative prompts to produce engaging videos.
      </p>
      <p>
        For this track, we will have the following set of of topics:
      </p>
      <ol>
        <li>
          People
        </li>
        <li>
          Animals
        </li>
        <li>
          Landscape
        </li>
      </ol>

      <H5>Evaluation Protocol</H5>


      <p>
        All submitted video/images will be uploaded to our crowdsourcing platform for public voting. The top three submissions with the highest votes for each topic will be declared winners. Beyond the winners per topic, we also have joint submission winners where we would have the best overall performing solution. We would use the Elo ranking system to compute the final ranking between different submissions.
      </p>


      <p>
        For the voting setup, we would use a pairwise comparison setup, showing two videos or images along with the topic, and ask the question "Which one would you prefer?". The selections would be 3-scale win/tie/lose rating.
      </p>

      <H5 id="challenge_a_instruction">Submission Instruction</H5>
      For this challenge, we do not have much limitations excepts: categories (see above) and the video length (less than 10s for Track A.1, 5mins for Track A.2). Please upload your videos or images indicating the category and track. We do not limit the number of submissions of each team.


    <H4>Challenge B -  Results Prediction with Auto Evaluation (Artifacts/Flaws)</H4>

    In this challenge, we will use provided (text, image) pairs and (text, video) pairs for which participants would run their own auto evaluation. Participants will submit a binary classification of these media indicating if the media (image or video) has flaws/artifacts.

      <H5>Track B.1 - Video Generation Auto Eval </H5>
      <p>
       For this track, we use Movie Gen Video Bench for benchmarking. Each participant will be asked to download the 1003 videos and prompts from Movie Gen Video Bench and run their auto eval models to classify the 1003 videos if they have artifacts or flaws.
      </p>

      <H5>Track B.2 - Image Generation Auto Eval </H5>

      <p>
        For this track, we use Emu_1k for benchmarking. Each participant can download the 1000 emu-generated images and prompts from the benchmark and run their auto eval models to indicate if there are flaws or aritifacts in each image.
      </p>


      <H5>Evaluation Protocol</H5>


      <p>
        All submitted video/images will be evaluated against our internal human annotations.
      </p>

      <H5 id="challenge_b_instruction">Submission Instruction</H5>
      <p>
          To participate in Track B.1, please follow these steps:
      </p>
      <ol>
          <li>
              Download the 1003 videos, indexed from 0 to 1002, from the
              <a href="https://github.com/facebookresearch/MovieGenBench?tab=readme-ov-file#movie-gen-video-bench" target="_blank" class="highlight-link">Movie Gen Video Bench</a>.
          </li>
          <li>
              Run your auto-evaluation model to generate the rankings for these videos. You can also utilize the prompts and meta information provided in the Movie Gen Video Generation benchmark to enhance your evaluation.
          </li>
          <li>
              To submit your results, prepare a text file with 1003 lines. Each line indicates whether this video has flaws or artifacts in the generation.
          </li>
      </ol>

      Example of a submitted txt file.
      <pre>
1
0
...
1
      </pre>
      In this submission,
      <ul>
        <li>The first video (<code>0.mp4</code>) has flaws or artifacts, so the first line is <code>1</code>.</li>
        <li>The second video (<code>1.mp4</code>) has no flaws, so the second line is <code>0</code>.</li>
        <li>The last video (<code>1002.mp4</code>) has artifacts, so the last line is <code>1</code>.</li>
    </ul>
<p><br /></p>
      <p>
          For Track B.2, please follow these instructions:
      </p>
      <ol>
          <li>
              Download the "images.zip" file from the
              <a href="https://huggingface.co/datasets/jhou90/GenAI_Emu_1k/tree/main" target="_blank" class="highlight-link">Emu 1k benchmark</a>.
          </li>
          <li>
              Inside "images.zip", you will find 1000 images along with their corresponding prompts. For example, "000000.jpg" has its prompts in "000000.txt".
          </li>
          <li>
              To submit your results, create a text file with 1000 lines. Each line should indicate whether this image has flaws or artifacts.
          </li>
      </ol>

      Example of a submitted txt file.
      <pre>
1
0
...
1
      </pre>
      In this submission,
      <ul>
        <li>The first image (<code>000000.jpg</code>) has flaws, so the first line is <code>1</code>.</li>
        <li>The second image (<code>000001.jpg</code>) has no flaws, so the second line is <code>0</code>.</li>
        <li>The last image (<code>000999.jpg</code>) has flaws, so the last line is <code>1</code>.</li>
    </ul>


<p><br /></p>
<div class="row" id="leaderboard">
  <div class="col-xs-12">
    <h2>Leaderboard</h2>
  </div>
</div>
<div class="row">
  <div class="col-md-12">
    TBA
   <!-- <table>
      <tbody>
      <tr><td><a href="https://arxiv.org/abs/2212.01558">#1. Emu</a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Zuck, Boz</font></td></tr>
      <tr><td><a href="https://arxiv.org/abs/2112.08359">#2. EmuV2</a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Zuck, Ahmad</font></td></tr>
      <tr><td><a href="https://arxiv.org/abs/2211.11682">#3. EmuV3</a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Boz, Xiaoliang</font></td> </tr>
      <tr><td><a href="https://arxiv.org/abs/2211.11682">#3. EmuV4</a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Kunkun</font></td> </tr>

    </tbody></table>
    -->
  </div>

</div>

<!--
<p><br /></p>
<div class="row" id="winners">
  <div class="col-xs-12">
    <h2>Winners</h2>
  </div>
</div>
TBD -->

<p><br /></p>

<div class="row" id="dates">
  <div class="col-xs-12">
    <h2>Important Dates</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
   <table class="table table-striped">
      <tbody>
        <tr>
          <td>Description</td>
          <td>Date</td>
        </tr>
        <tr>
          <td>Submission opens for all Challenges.</td>
          <td>3/3/2025</td>
        </tr>
        <tr>
          <td>Submission closes for Challenge A.</td>
          <td>4/14/2025</td>
        </tr>
        <tr>
          <td>Crowd-sourced polling opens</td>
          <td>4/21/2025</td>
        </tr>
        <tr>
          <td>Submission closes for Challenge B.</td>
          <td>6/2/2025</td>
        </tr>
        <tr>
          <td>Crowd-sourced poll ends.</td>
          <td>6/2/2025</td>
        </tr>
        <tr>
          <td>Workshop Date</td>
          <td>6/11/2025</td>
        </tr>

      </tbody>
    </table>

  </div>
</div>

<p><br /></p>
<div class="row" id="schedule">
  <div class="col-xs-8">
    <h2>Workshop Schedule</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
  <table class="table table-striped">
  <thead>
    <tr>
      <th></th>
      <th>Time</th>
      <th>Agenda</th>
      <th>Speech Title</th>
      <th>Speaker(s)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Opening Session</td>
      <td>09:15 - 09:30</td>
      <td>Opening Remarks</td>
      <td></td>
      <td>Ji Hou</td>
    </tr>
    <tr>
      <td rowspan="2">Session I</td>
      <td>09:30 - 10:00</td>
      <td>Keynote Speech</td>
      <td>TBA</td>
      <td>Björn Ommer</td>
    </tr>
    <tr>
      <td>10:00 - 10:30</td>
      <td>Keynote Speech</td>
      <td>TBA</td>
      <td>Haoqi Fan</td>
    </tr>
    <tr>
      <td>Break</td>
      <td>10:30 - 10:50</td>
      <td colspan="3">Coffee Break</td>
    </tr>
    <tr>
      <td rowspan="2">Session II</td>
      <td>10:50 - 11:20</td>
      <td>Keynote Speech</td>
      <td>Still Training GANs in 2025?</td>
      <td>Jun-Yan Zhu</td>
    </tr>
    <tr>
      <td>11:20 - 11:50</td>
      <td>Keynote Speech</td>
      <td>TBA</td>
      <td>Saining Xie</td>
    </tr>
    <tr>
      <td>11:50 - 12:20</td>
      <td>Keynote Speech</td>
      <td colspan="2">The Canvas of You: Visual Personalization in Space and Time</td>
      <td>Sergey Tulyakov</td>
    </tr>
    <tr>
      <td>Closing Session</td>
      <td>12:20 - 12:30</td>
      <td colspan="2">Closing Remarks</td>
      <td>Yaqiao Luo</td>
    </tr>
  </tbody>
</table>
  </div>
</div>

<p><br /></p>

<div class="row" id="speakers">
  <div class="col-xs-12">
    <h2>Invited Speakers</h2>
  </div>
</div>

<div class="row">
  <div class="col-md-2">
    <a href=""><img class="speaker-pic" style="float:left;margin-right:50px;" src="static/img/people/bjoern_ommer.jpg" /></a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://ommer-lab.com/people/ommer/">Björn Ommer</a></b> Dr. Björn Ommer is a full professor at LMU where he heads the Computer Vision & Learning Group (previously Computer Vision Group Heidelberg). Before he was a full professor at the Department of Mathematics and Computer Science of Heidelberg University and also served as a one of the directors of the Interdisciplinary Center for Scientific Computing (IWR) and of the Heidelberg Collaboratory for Image Processing (HCI). He has served as program chair for GCPR, as Senior Area Chair and Area Chair for multiple CVPR, ICCV, ECCV, and NeurIPS conferences, and as workshop and tutorial organizer at these venues.   </p>
  </div>
</div>

<p><br /></p>

<div class="row">
  <div class="col-md-2">
    <a href=""><img class="speaker-pic" style="float:left;margin-right:50px;" src="static/img/people/junyan_zhu.jpg" /></a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a></b> Dr. Jun-Yan Zhu is the Michael B. Donohue Assistant Professor of Computer Science and Robotics at CMU’s School of Computer Science. Prior to joining CMU, he was a Research Scientist at Adobe Research and a postdoc at MIT CSAIL. He obtained his Ph.D. from UC Berkeley and B.E. from Tsinghua University. He studies computer vision, computer graphics, and computational photography. His current research focuses on generative models for visual storytelling. He is the recipient of the Samsung AI Research of the Year, the Packard Fellowships for Science and Engineering, the NSF CAREER Award, the ACM SIGGRAPH Outstanding Doctoral Dissertation Award, and the UC Berkeley EECS David J. Sakrison Memorial Prize for outstanding doctoral research, among other awards.
    </p>
  </div>
</div>

<p><br /></p>

    <div class="row">
  <div class="col-md-2">
    <a href=""><img class="speaker-pic" style="float:left;margin-right:50px;" src="static/img/people/sergey_tulyakov.jpg" /></a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="http://www.stulyakov.com/">Sergey Tulyakov</a></b> Dr. Sergey Tulyakov is the Director of Research at Snap Inc., where he leads the Creative Vision team. Sergey’s work focuses on building technology to enhance creators’ skills using computer vision, machine learning, and generative AI. His work involves 2D, 3D, video generation, editing, and personalization. To scale generative experiences to hundreds of millions of users, Sergey’s team builds the world’s most efficient mobile foundational models, which enhance multiple products at Snap Inc. Sergey pioneered video generation and unsupervised animation domains with MoCoGAN, MonkeyNet, and the First Order Motion Model, sparking several startups in the field. His work on Interactive Video Stylization received the Best in Show Award at SIGGRAPH Real-Time Live! 2020. He has published over 60 top conference papers, journals, and patents, resulting in multiple innovative products, including Real-time Neural Lenses, real-time try-on, Snap AI Video, Imagine Together, world's fastest foundational image-to-image model and many more. Before joining Snap Inc., Sergey was with Carnegie Mellon University, Microsoft, and NVIDIA. He holds a PhD from the University of Trento, Italy. 
  </div>

    <div class="row">
  <div class="col-md-2">
    <a href=""><img class="speaker-pic" style="float:left;margin-right:50px;" src="static/img/people/haoqi_fan.jpg" /></a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://haoqifan.github.io/">Haoqi Fan</a></b> Haoqi Fan is a Research Scientist at Seed Edge, where he leads efforts to build world foundational models. He spent seven years at Facebook AI Research (FAIR), focusing on self-supervised learning and backbone design for image and video understanding. His works won the ActivityNet Challenge at ICCV 2019 and were nominated for Best Paper at CVPR 2020. He has also co-organized several tutorials at CVPR, ICCV, and ECCV.
  </div>
</div>

<p><br /></p>

  <div class="row">
  <div class="col-md-2">
    <a href=""><img class="speaker-pic" style="float:left;margin-right:50px;" src="static/img/people/saining_xie.png" /></a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://www.sainingxie.com/">Saining Xie</a></b> Dr. Saining Xie is an Assistant Professor of Computer Science at NYU Courant and part of the CILVR group. He is also affiliated with NYU Center for Data Science. Before that I was a research scientist at Facebook AI Research (FAIR), Menlo Park. He received my Ph.D. and M.S. degrees from CSE Department at UC San Diego, advised by Zhuowen Tu. During his PhD study, he also interned at NEC Labs, Adobe, Facebook, Google, DeepMind. Prior to that, he obtained his bachelor degree from Shanghai Jiao Tong University. His primary areas of interest in research are computer vision and machine learning.
</div>


</div>
<!---

<div class="row">
  <div class="col-md-2">
    <a href=""><img class="speaker-pic" style="float:left;margin-right:50px" src="static/img/people/yuanzhen_li.jpg" /></a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://people.csail.mit.edu/yzli/">Yuanzhen Li</a></b> At Google Research, Yuanzhen supports a talented team and a few cross-team Computer Vision and Generative AI efforts. Recent work includes: Generative Product Imagery, "Imagen" in Google Cloud Vertex AI, Muse, DreamBooth, DreamBooth3D, Generative Uncrop, etc. Prior to Google, she spent 10 years in the startup world. She made iPhone computational-photography apps, and one of them (TrueHDR) was quite popular in the 2009-2012 era. She also founded a startup leveraging deep learning for image search; it was acquired by VSCO in 2015 and I then worked at VSCO as a Director of Engineering. Prior to that, she completed my PhD in 2009 at MIT with Prof. Edward H. Adelson. At MIT, she was a member of the Computer Science and Artificial Intelligence Laboratory (CSAIL) and the Perceptual Science Laboratory.
    </p>
  </div>
</div>

<p><br /></p>

<div class="row">
  <div class="col-md-2">
    <a href=""><img class="speaker-pic" style="float:left;margin-right:50px;" src="static/img/people/richard_zhang.jpg" /></a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://richzhang.github.io/">Richard Zhang</a></b> Dr. Richard Zhang’s research interests are in computer vision, machine learning, deep
      learning, graphics, and image processing. He obtained a PhD at UC Berkeley, advised by
      Prof. Alexei (Alyosha) Efros. He obtained BS and MEng degrees from Cornell University
      in ECE. He often collaborates with academic researchers, either through internships or
      university collaboration. Recently, he was included on MIT Technology Review's list of 35
      Innovators Under 35.
  </div>
</div>


<p><br /></p>

    <div class="row">
  <div class="col-md-2">
    <a href=""><img class="speaker-pic" style="float:left;margin-right:50px;" src="static/img/people/tim_salimans.jpg" /></a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://research.google/people/tim-salimans/">Tim Salimans</a></b> Tim is a Machine Learning research scientist working on generative modeling. He is well known for his work on GANs and VAEs, and their evaluation using the Inception score, as well as his work on autoregressive generative models like GPT-1 and PixelCNN++. More recently, he has been focusing on diffusion models for generating images (Imagen) and video (Imagen Video), and on making these models fast to sample using distillation.
  </div>

</div>

-->


<p><br /></p>
<div class="row" id="organizers">
  <div class="col-xs-12">
    <h2>Organizers</h2>
  </div>
</div>

<div class="row">


  <div class="col-xs-2">
    <a href="https://sekunde.github.io">
      <img class="people-pic" src="static/img/people/ji_hou.jpg" />
    </a>
    <div class="people-name">
      <a href="https://sekunde.github.io/">Ji Hou</a>
      <h6>Meta AI</h6>
    </div>
  </div>

 <div class="col-xs-2">
    <a href="https://scholar.google.com/citations?user=JdE_LFYAAAAJ&hl=en">
      <img class="people-pic" src="static/img/people/sam_tsai.jpg" />
    </a>
    <div class="people-name">
      <a href="https://scholar.google.com/citations?user=JdE_LFYAAAAJ&hl=en">Sam Tsai</a>
      <h6>Meta AI</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://www.linkedin.com/in/ajay-menon-022650267">
      <img class="people-pic" src="static/img/people/yaqiao_luo.jpg" />
    </a>
    <div class="people-name">
      <a href="https://www.linkedin.com/in/ajay-menon-022650267">Yaqiao Luo</a>
      <h6>Meta AI</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://www.linkedin.com/in/ajay-menon-022650267">
      <img class="people-pic" src="static/img/people/luxin_zhang.jpg" />
    </a>
    <div class="people-name">
      <a href="https://www.linkedin.com/in/ajay-menon-022650267">Luxin Zhang</a>
      <h6>Meta AI</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://kunpengli1994.github.io/">
      <img class="people-pic" src="static/img/people/kunpeng_li.jpg" />
    </a>
    <div class="people-name">
      <a href="https://kunpengli1994.github.io/">Kunpeng Li</a>
      <h6>Meta AI</h6>
    </div>
  </div>


  <div class="col-xs-2">
    <a href="https://www.linkedin.com/in/ajay-menon-022650267">
      <img class="people-pic" src="static/img/people/maxime.jpg" />
    </a>
    <div class="people-name">
      <a href="https://www.linkedin.com/in/ajay-menon-022650267">Maxime Seknadje</a>
      <h6>Meta AI</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://sites.google.com/view/xiaoliangdai/">
      <img class="people-pic" src="static/img/people/xiaoliang_dai.jpg" />
    </a>
    <div class="people-name">
      <a href="https://sites.google.com/view/xiaoliangdai/">Xiaoliang Dai</a>
      <h6>Meta AI</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://chihyaoma.github.io">
      <img class="people-pic" src="static/img/people/kevin_ma.jpg" />
    </a>
    <div class="people-name">
      <a href="https://chihyaoma.github.io/">Kevin Chih-Yao Ma</a>
      <h6>Meta AI</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://openreview.net/profile?id=~Matthew_Yu2">
      <img class="people-pic" src="static/img/people/matthew_yu.jpg" />
    </a>
    <div class="people-name">
      <a href="https://openreview.net/profile?id=~Matthew_Yu2">Matthew Yu</a>
      <h6>Meta AI</h6>
    </div>
  </div>


  <div class="col-xs-2">
    <a href="https://www.linkedin.com/in/tianhe-li/">
      <img class="people-pic" src="static/img/people/tianhe_li.jpg" />
    </a>
    <div class="people-name">
      <a href="https://www.linkedin.com/in/tianhe-li/">Tianhe Li</a>
      <h6>Meta AI</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://www.linkedin.com/in/simran-motwani">
      <img class="people-pic" src="static/img/people/simran_motwani.jpg" />
    </a>
    <div class="people-name">
      <a href="https://www.linkedin.com/in/simran-motwani">Simran Motwani</a>
      <h6>Meta AI</h6>
    </div>
  </div>


  <div class="col-xs-2">
    <a href="https://sites.google.com/view/taoxu">
      <img class="people-pic" src="static/img/people/tao_xu.jpg" />
    </a>
    <div class="people-name">
      <a href="https://sites.google.com/view/taoxu">Tao Xu</a>
      <h6>Meta AI</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://sites.google.com/view/jialiangwang/home">
      <img class="people-pic" src="static/img/people/jialiang_wang.jpg" />
    </a>
    <div class="people-name">
      <a href="https://sites.google.com/view/jialiangwang/home">Jialiang Wang</a>
      <h6>Meta AI</h6>
    </div>
  </div>

    <div class="col-xs-2">
    <a href="https://www.linkedin.com/in/karthiksivakumar/">
      <img class="people-pic" src="static/img/people/karthik_sivakumar.jpg" />
    </a>
    <div class="people-name">
      <a href="https://www.linkedin.com/in/karthiksivakumar/">Karthik Sivakumar</a>
      <h6>Meta AI</h6>
    </div>
  </div>


</div>
<p><br /></p>
<p><br /></p>

<div class="row" id="advisors">
  <div class="col-xs-12">
    <h2>Senior Advisors</h2>
  </div>
</div>

<div class="row">

  <div class="col-xs-2">
    <a href="https://sites.google.com/site/vajdap">
      <img class="people-pic" src="static/img/people/peter_vajda.jpg" />
    </a>
    <div class="people-name">
      <a href="https://sites.google.com/site/vajdap">Peter Vajda</a>
      <h6>Meta AI</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://scholar.google.com/citations?user=eqQQkM4AAAAJ&hl=en">
      <img class="people-pic" src="static/img/people/peizhao_zhang.jpg" />
    </a>
    <div class="people-name">
      <a href="https://scholar.google.com/citations?user=eqQQkM4AAAAJ&hl=en">Peizhao Zhang</a>
      <h6>Meta AI</h6>
    </div>
  </div>

    <div class="col-xs-2">
    <a href="http://www.stulyakov.com/">
      <img class="people-pic" src="static/img/people/sergey_tulyakov.jpg" />
    </a>
    <div class="people-name">
      <a href="http://www.stulyakov.com/">Sergey Tulyakov</a>
      <h6>Snap</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://scholar.google.com/citations?user=G03EzSMAAAAJ&hl=en">
      <img class="people-pic" src="static/img/people/zijian_he.jpg" />
    </a>
    <div class="people-name">
      <a href="https://scholar.google.com/citations?user=G03EzSMAAAAJ&hl=en">Zijian He</a>
      <h6>Meta AI</h6>
    </div>
  </div>


  <div class="col-xs-2">
    <a href="https://www.alithabet.com/">
      <img class="people-pic" src="static/img/people/ali_thabet.jpg" />
    </a>
    <div class="people-name">
      <a href="https://www.alithabet.com/">Ali Thabet</a>
      <h6>Meta AI</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://ai.meta.com/people/roshan-sumbaly/">
      <img class="people-pic" src="static/img/people/roshan_sumbaly.jpg" />
    </a>
    <div class="people-name">
      <a href="https://ai.meta.com/people/roshan-sumbaly/">Roshan Sumbaly</a>
      <h6>Meta AI</h6>
    </div>
  </div>


</div>

<p><br /></p>

<div class="row" id="contact">
  <div class="col-xs-12">
    <h2>Contact</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      To contact the organizers please use <b>gen_ai_media_generation_challenge_cvpr_workshop@meta.com</b>
    </p>
  </div>
</div>
<p><br /></p>

<hr />

<div class="row">
  <div class="col-xs-12">
    <h2>Acknowledgments</h2>
  </div>
</div>
<p><a name="/acknowledgements"></a></p>
<div class="row">
  <div class="col-xs-12">
    <p>
      Thanks to <span style="color:#1a1aff;font-weight:400;"> <a href="https://languagefor3dscenes.github.io/">languagefor3dscenes</a></span> for the webpage format.
    </p>
  </div>
</div>

      </div>
    </div>

  </body>
</html>
